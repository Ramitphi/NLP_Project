# -*- coding: utf-8 -*-
"""NLP_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12gDkWn1J8k87VtSXjfigPrCNUEyKfQ6f

#Loading the Books
"""

b1 = open("14249.txt", "r")

T1= b1.read()

T1 #first book that we have used

b2 = open("34861.txt", "rt")

T2=b2.read()
T2 #second book that we have used

"""To remove the upper section containng the details of project gutenrberg and removing the lower section containing the lisence,certification etc"""

T1.find("CHAPTER")   #first found the index of the starting of chapter I

T1.find("End of the Project") #then we will find the index of the starting of the lower part of the book

resT1 = T1[1630:366315] #we will use slicing to slice text between those indexes

resT1

"""Simillar index finding technique used for book T2"""

indext2_1= T2.find("CHAPTER") 
indext2_1

index2_2=T2.find("in--his.")
index2_2

resT2 = T2[2500:469306]
resT2

resT1

"""After removing the header and footer information from text we will apply other text processing thing to the book text"""

import re

resT1=resT1.lower() #lower the text

resT2=resT2.lower()

resT1 = re.sub(r'chapter \w+', '', resT1) #removes the running chapter names

resT1

resT1 = re.sub(r'[^a-zA-Z0-9\s]', '', resT1) #it will remove some puncuation and special characters

resT1

resT1 = re.findall(r'\w+', resT1) #will output the words that are matching the regex pattern and will output a list

tokensT1 = resT1

tokensT1[:16]

type(tokensT1)

"""Simillar kind of operation will be performed on the book T2"""

resT2 = re.sub(r'[^a-zA-Z0-9\s]', '', resT2)
resT2 = re.sub(r'chapter \w+', '', resT2)
resT2 = re.findall(r'\w+', resT2) #will output the words that are matching the regex pattern and will output a list
resT2[:10]

tokensT2 = resT2

print(len(tokensT1))
print(len(tokensT2))

"""#Analyzing the frequency distribution of Tokens

####T1
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

t1tokens, t1counts = np.unique(tokensT1, return_counts=True)

#coouting the occurence of unique tokens

t1counts

t1tokens

t1tokencountdf = pd.DataFrame(list(zip(t1tokens, t1counts)), 
               columns =['Token', 'Counts'])  #using the zip function we zipped the two list together and made a dataframe from it

t1tokencountdf.head()

t1tokencountdf.sort_values(by=['Counts'], ascending=False,inplace=True) #we will be sorting it in descneding order to see the tokens with maximum count

t1tokencountdf.head()

t1df= t1tokencountdf[:25] #considering only top 25 tokens

t1df.plot(x= 'Token',y='Counts',kind='barh',figsize=(8, 6))

"""###T2"""

t2tokens, t2counts = np.unique(tokensT2, return_counts=True)

t2tokencountdf = pd.DataFrame(list(zip(t2tokens, t2counts)), 
               columns =['Token', 'Counts'])

t2tokencountdf.head()

t2tokencountdf.sort_values(by=['Counts'], ascending=False,inplace=True)

t2tokencountdf.head()

t2df= t2tokencountdf[:25]

t2df.plot(x= 'Token',y='Counts',kind='barh',figsize=(8, 6))

#with stopword for tokens t1

from wordcloud import WordCloud, STOPWORDS 
import matplotlib.pyplot as plt 
import pandas as pd 




comment_words = '' 
stopwords = []  #we do not want to remove stopwords so we have kept the list empty

tokens = tokensT1

comment_words += " ".join(tokens)+" "

wordcloud = WordCloud(width = 800, height = 800, 
				background_color ='white', 
				stopwords = stopwords, 
				min_font_size = 10).generate(comment_words) 

# plot the WordCloud image					 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wordcloud) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()

#with stopword for tokens t2
comment_words = '' 
stopwords = set(STOPWORDS)

tokens = tokensT2

comment_words += " ".join(tokens)+" "

wordcloud = WordCloud(width = 800, height = 800, 
				background_color ='white', 
				stopwords = [], 
				min_font_size = 10).generate(comment_words) 

# plot the WordCloud image					 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wordcloud) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()

"""#Removing stopwords from T1 and T2"""

import nltk
from nltk.corpus import stopwords 
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

tokensT1withoutstop = [w for w in tokensT1 if not w in stop_words]

tokensT2withoutstop = [w for w in tokensT2 if not w in stop_words]

tokensT1withoutstop[:10]

tokensT2withoutstop[:10]

#without stopword for tokens t1

comment_words = '' 


tokens =tokensT1withoutstop

comment_words += " ".join(tokens)+" "

wordcloud = WordCloud(width = 800, height = 800, 
				background_color ='white',  
				min_font_size = 10).generate(comment_words) 

# plot the WordCloud image					 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wordcloud) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()

#without stopword for tokens t1

comment_words = '' 


tokens = tokensT2withoutstop

comment_words += " ".join(tokens)+" "

wordcloud = WordCloud(width = 800, height = 800, 
				background_color ='white',  
				min_font_size = 10).generate(comment_words) 

# plot the WordCloud image					 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wordcloud) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()

"""#Relationship between Word Length & Frequency

###Tokens with stopwords
"""

import seaborn as sns

tokensT1df=pd.DataFrame(tokensT1)
tokensT2df=pd.DataFrame(tokensT2)

tokensT1df.head()

tokensT2df.head()

tokensT1df=pd.DataFrame(tokensT1)

tokensT2df=pd.DataFrame(tokensT2)
freqt1=tokensT1df.value_counts()
freqt1= pd.DataFrame(freqt1,columns=['freq'])
freqt1w= freqt1.reset_index()

freqt1=tokensT1df.value_counts()

print(freqt1)

freqt2=tokensT2df.value_counts()
print(freqt2)

freqt1= pd.DataFrame(freqt1,columns=['freq'])
freqt1w= freqt1.reset_index()

freqt1w=freqt1w.rename(columns={0: "tokens", "freq": "frequency"})

freqt1w.head()

freqt1w['length'] = freqt1w['tokens'].apply(lambda x: len(x))  #using the apply method to apply the len() function to all values of a column

freqt1w.head()

freqt2w= pd.DataFrame(freqt2,columns=['freq'])

freqt2w=freqt2w.reset_index()

freqt2w=freqt2w.rename(columns={0: "tokens", "freq": "frequency"})

freqt2w['length'] = freqt2w['tokens'].apply(lambda x: len(x))

freqt2w.head()

plt.scatter(freqt1w['length'], freqt1w['frequency'])

plt.title("Relationship between the word length & frequency for T1(with stopwords) ")
plt.ylabel("Frequency")
plt.xlabel("Length")

plt.scatter(freqt2w['length'], freqt2w['frequency'])
plt.title("Relationship between the word length & frequency for T2(with stopwords) ")
plt.ylabel("Frequency")
plt.xlabel("Length")

tokensT1dfs=pd.DataFrame(tokensT1withoutstop)
tokensT2dfs=pd.DataFrame(tokensT2withoutstop)

freqt1s=tokensT1dfs.value_counts()

print(freqt1s)

freqt2s=tokensT2dfs.value_counts()
print(freqt2s)

freqt1s= pd.DataFrame(freqt1s,columns=['freq'])
freqt1sw= freqt1s.reset_index()
freqt1sw=freqt1sw.rename(columns={0: "tokens", "freq": "frequency"})

freqt1sw.head()

freqt1sw['length'] = freqt1sw['tokens'].apply(lambda x: len(x))
freqt1sw.head()

freqt2s= pd.DataFrame(freqt2s,columns=['freq'])
freqt2sw= freqt2s.reset_index()
freqt2sw=freqt2sw.rename(columns={0: "tokens", "freq": "frequency"})

freqt2sw.head()

freqt2sw['length'] = freqt2sw['tokens'].apply(lambda x: len(x))
freqt2sw.head()

plt.scatter(freqt1sw['length'], freqt1sw['frequency'])
plt.title("Relationship between the word length & frequency for T1 ")
plt.ylabel("Frequency")
plt.xlabel("Length")

plt.scatter(freqt2sw['length'], freqt2sw['frequency'])
plt.title("Relationship between the word length & frequency for T2 ")
plt.ylabel("Frequency")
plt.xlabel("Length")

import nltk

nltk.download('averaged_perceptron_tagger')

tags_T1 = nltk.pos_tag(tokensT1)

tag1 = [ t for (w,t) in tags_T1]

tag1[:10]

t1tags, t1tagcount = np.unique(tag1, return_counts=True)
t1tagcountdf = pd.DataFrame(list(zip(t1tags, t1tagcount)), 
               columns =['Tags', 'Counts'])

t1tagcountdf.head()

t1tagcountdf.sort_values(by=['Counts'], ascending=False,inplace=True)

t1toptagdf= t1tagcountdf[:30]
t1toptagdf.plot(x= 'Tags',y='Counts',kind='barh',figsize=(8, 6))

tags_T2=nltk.pos_tag(tokensT2)

tags_T2[:20]

tags2 = [ t for (w,t) in tags_T2]

tags2[:10]

t2tags, t2tagcount = np.unique(tags2, return_counts=True)
t2tagcountdf = pd.DataFrame(list(zip(t2tags, t2tagcount)), 
               columns =['Tags', 'Counts'])

t2tagcountdf.head()

t2tagcountdf.sort_values(by=['Counts'], ascending=False,inplace=True)

t2toptagdf= t2tagcountdf[:30]
t2toptagdf.plot(x= 'Tags',y='Counts',kind='barh',figsize=(8, 6))

