# -*- coding: utf-8 -*-
"""NLP_project_complete.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12gDkWn1J8k87VtSXjfigPrCNUEyKfQ6f

#Loading the Books
"""

b1 = open("14249.txt", "r")

T1= b1.read()

T1 #first book that we have used

b2 = open("34861.txt", "rt")

T2=b2.read()
T2 #second book that we have used

"""To remove the upper section containng the details of project gutenrberg and removing the lower section containing the lisence,certification etc  

"""

T1.find("CHAPTER")   #first found the index of the starting of chapter I

T1.find("End of the Project") #then we will find the index of the starting of the lower part of the book

resT1 = T1[1630:366315] #we will use slicing to slice text between those indexes

resT1

"""Simillar index finding technique used for book T2"""

indext2_1= T2.find("CHAPTER") 
indext2_1

index2_2=T2.find("in--his.")
index2_2

resT2 = T2[2500:469306]
resT2

resT1

"""After removing the header and footer information from text we will apply other text processing thing to the book text"""

import re

resT1=resT1.lower() #lower the text

resT2=resT2.lower()

resT1 = re.sub(r'chapter \w+', '', resT1) #removes the running chapter names

resT1

resT1 = re.sub(r'[^a-zA-Z0-9\s]', '', resT1) #it will remove some puncuation and special characters

resT1

nerT1 = resT1

resT1 = re.findall(r'\w+', resT1) #will output the words that are matching the regex pattern and will output a list

tokensT1 = resT1

tokensT1[:16]

type(tokensT1)

"""Simillar kind of operation will be performed on the book T2"""

resT2

resT2 = re.sub(r'chapter \w+', '', resT2)
resT2 = re.sub(r'[^a-zA-Z0-9\s]', ' ', resT2)

resT2

nerT2=resT2
resT2 = re.findall(r'\w+', resT2)

resT2[:10]

tokensT2 = resT2

tokensT2[:10]

print(len(tokensT1))
print(len(tokensT2))

"""#Analyzing the frequency distribution of Tokens

####T1
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

t1tokens, t1counts = np.unique(tokensT1, return_counts=True)

#coouting the occurence of unique tokens

t1counts

t1tokens

t1tokencountdf = pd.DataFrame(list(zip(t1tokens, t1counts)), 
               columns =['Token', 'Counts'])  #using the zip function we zipped the two list together and made a dataframe from it

t1tokencountdf.head()

t1tokencountdf.sort_values(by=['Counts'], ascending=False,inplace=True) #we will be sorting it in descneding order to see the tokens with maximum count

t1tokencountdf.head()

t1df= t1tokencountdf[:25] #considering only top 25 tokens

t1df.plot(x= 'Token',y='Counts',kind='barh',figsize=(8, 6))

"""###T2"""

t2tokens, t2counts = np.unique(tokensT2, return_counts=True)

t2tokencountdf = pd.DataFrame(list(zip(t2tokens, t2counts)), 
               columns =['Token', 'Counts'])

t2tokencountdf.head()

t2tokencountdf.sort_values(by=['Counts'], ascending=False,inplace=True)

t2tokencountdf.head()

t2df= t2tokencountdf[:25]

t2df.plot(x= 'Token',y='Counts',kind='barh',figsize=(8, 6))

#with stopword for tokens t1

from wordcloud import WordCloud, STOPWORDS 
import matplotlib.pyplot as plt 
import pandas as pd 




comment_words = '' 
stopwords = []  #we do not want to remove stopwords so we have kept the list empty

tokens = tokensT1

comment_words += " ".join(tokens)+" "

wordcloud = WordCloud(width = 800, height = 800, 
				background_color ='white', 
				stopwords = stopwords, 
				min_font_size = 10).generate(comment_words) 

# plot the WordCloud image					 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wordcloud) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()

#with stopword for tokens t2
comment_words = '' 
stopwords = set(STOPWORDS)

tokens = tokensT2

comment_words += " ".join(tokens)+" "

wordcloud = WordCloud(width = 800, height = 800, 
				background_color ='white', 
				stopwords = [], 
				min_font_size = 10).generate(comment_words) 

# plot the WordCloud image					 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wordcloud) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()

"""#Removing stopwords from T1 and T2"""

import nltk
from nltk.corpus import stopwords 
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

tokensT1withoutstop = [w for w in tokensT1 if not w in stop_words]

tokensT2withoutstop = [w for w in tokensT2 if not w in stop_words]

tokensT1withoutstop[:10]

tokensT2withoutstop[:10]

#without stopword for tokens t1

comment_words = '' 


tokens =tokensT1withoutstop

comment_words += " ".join(tokens)+" "

wordcloud = WordCloud(width = 800, height = 800, 
				background_color ='white',  
				min_font_size = 10).generate(comment_words) 

# plot the WordCloud image					 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wordcloud) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()

#without stopword for tokens t2

comment_words = '' 


tokens = tokensT2withoutstop

comment_words += " ".join(tokens)+" "

wordcloud = WordCloud(width = 800, height = 800, 
				background_color ='white',  
				min_font_size = 10).generate(comment_words) 

# plot the WordCloud image					 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wordcloud) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()

"""#Relationship between Word Length & Frequency

###Tokens with stopwords
"""

import seaborn as sns

tokensT1df=pd.DataFrame(tokensT1)
tokensT2df=pd.DataFrame(tokensT2)

tokensT1df.head()

tokensT2df.head()

tokensT1df=pd.DataFrame(tokensT1)

tokensT2df=pd.DataFrame(tokensT2)
freqt1=tokensT1df.value_counts()
freqt1= pd.DataFrame(freqt1,columns=['freq'])
freqt1w= freqt1.reset_index()

freqt1=tokensT1df.value_counts()

print(freqt1)

freqt2=tokensT2df.value_counts()
print(freqt2)

freqt1= pd.DataFrame(freqt1,columns=['freq'])
freqt1w= freqt1.reset_index()

freqt1w=freqt1w.rename(columns={0: "tokens", "freq": "frequency"})

freqt1w.head()

freqt1w['length'] = freqt1w['tokens'].apply(lambda x: len(x))  #using the apply method to apply the len() function to all values of a column

freqt1w.head()

freqt2w= pd.DataFrame(freqt2,columns=['freq'])

freqt2w=freqt2w.reset_index()

freqt2w=freqt2w.rename(columns={0: "tokens", "freq": "frequency"})

freqt2w['length'] = freqt2w['tokens'].apply(lambda x: len(x))

freqt2w.head()

plt.scatter(freqt1w['length'], freqt1w['frequency'])

plt.title("Relationship between the word length & frequency for T1(with stopwords) ")
plt.ylabel("Frequency")
plt.xlabel("Length")

plt.scatter(freqt2w['length'], freqt2w['frequency'])
plt.title("Relationship between the word length & frequency for T2(with stopwords) ")
plt.ylabel("Frequency")
plt.xlabel("Length")

tokensT1dfs=pd.DataFrame(tokensT1withoutstop)
tokensT2dfs=pd.DataFrame(tokensT2withoutstop)

freqt1s=tokensT1dfs.value_counts()

print(freqt1s)

freqt2s=tokensT2dfs.value_counts()
print(freqt2s)

freqt1s= pd.DataFrame(freqt1s,columns=['freq'])
freqt1sw= freqt1s.reset_index()
freqt1sw=freqt1sw.rename(columns={0: "tokens", "freq": "frequency"})

freqt1sw.head()

freqt1sw['length'] = freqt1sw['tokens'].apply(lambda x: len(x))
freqt1sw.head()

freqt2s= pd.DataFrame(freqt2s,columns=['freq'])
freqt2sw= freqt2s.reset_index()
freqt2sw=freqt2sw.rename(columns={0: "tokens", "freq": "frequency"})

freqt2sw.head()

freqt2sw['length'] = freqt2sw['tokens'].apply(lambda x: len(x))
freqt2sw.head()

plt.scatter(freqt1sw['length'], freqt1sw['frequency'])
plt.title("Relationship between the word length & frequency for T1 ")
plt.ylabel("Frequency")
plt.xlabel("Length")

plt.scatter(freqt2sw['length'], freqt2sw['frequency'])
plt.title("Relationship between the word length & frequency for T2 ")
plt.ylabel("Frequency")
plt.xlabel("Length")

import nltk

nltk.download('averaged_perceptron_tagger')

tags_T1 = nltk.pos_tag(tokensT1)

tag1 = [ t for (w,t) in tags_T1]

tag1[:10]

t1tags, t1tagcount = np.unique(tag1, return_counts=True)
t1tagcountdf = pd.DataFrame(list(zip(t1tags, t1tagcount)), 
               columns =['Tags', 'Counts'])

t1tagcountdf.head()

t1tagcountdf.sort_values(by=['Counts'], ascending=False,inplace=True)

t1toptagdf= t1tagcountdf[:30]
t1toptagdf.plot(x= 'Tags',y='Counts',kind='barh',figsize=(8, 6))

tags_T2=nltk.pos_tag(tokensT2)

tags_T2[:20]

tags2 = [ t for (w,t) in tags_T2]

tags2[:10]

t2tags, t2tagcount = np.unique(tags2, return_counts=True)
t2tagcountdf = pd.DataFrame(list(zip(t2tags, t2tagcount)), 
               columns =['Tags', 'Counts'])

t2tagcountdf.head()

t2tagcountdf.sort_values(by=['Counts'], ascending=False,inplace=True)

t2toptagdf= t2tagcountdf[:30]
t2toptagdf.plot(x= 'Tags',y='Counts',kind='barh',figsize=(8, 6))

tags_T1[:20]

tags_T1[0][1][0]

nouns_T1=[]
for i in tags_T1:
  if(i[1][0] == 'N'):
    nouns_T1.append(i[0])

nouns_T1[:10]

verb_T1=[]
for i in tags_T1:
  if(i[1][0] == 'V'):
    verb_T1.append(i[0])

verb_T1[:10]

tags_T2[:20]

nouns_T2=[]
for i in tags_T2:
  if(i[1][0] == 'N'):
    nouns_T2.append(i[0])

nouns_T2[:10]

verb_T2=[]
for i in tags_T2:
  if(i[1][0] == 'V'):
    verb_T2.append(i[0])

verb_T2[:10]

import nltk
nltk.download('wordnet')

from nltk.corpus import wordnet

nt1=[]

for i in nouns_T1:
  syn =  wordnet.synsets(i,pos=wordnet.NOUN)
  if len(syn)>0:
    nt1.append(syn[0].lexname())

print(list(zip(nouns_T1,nt1)))

vt1=[]

for i in verb_T1:
  syn =  wordnet.synsets(i,pos=wordnet.VERB)
  if len(syn)>0:
    vt1.append(syn[0].lexname())
#print(vt1)
print(list(zip(verb_T1,vt1)))

nt2=[]

for i in nouns_T2:
  syn =  wordnet.synsets(i,pos=wordnet.NOUN)
  if len(syn)>0:
    nt2.append(syn[0].lexname())
    
print(list(zip(nouns_T2,nt2)))

vt2=[]

for i in verb_T2:
  syn =  wordnet.synsets(i,pos=wordnet.VERB)
  if len(syn)>0:
    vt2.append(syn[0].lexname())
#print(vt2)
print(list(zip(verb_T2,vt2)))

import seaborn as sns

fig = plt.gcf()
fig.set_size_inches(15,5)
g = sns.histplot(x = nt1 , color = "red" , kde = True)
plt.setp(g.get_xticklabels() , rotation = 90)
plt.xlabel('Different Categories of Noun in Book 1')
plt.ylabel('Frequency of each category')
plt.title('Histogram of Nouns in Book 1');

fig = plt.gcf()
fig.set_size_inches(15,5)
g = sns.histplot(x = nt2 , color = "orange" , kde = True)
plt.setp(g.get_xticklabels() , rotation = 90)
plt.xlabel('Different Categories of Noun in Book 2')
plt.ylabel('Frequency of each category')
plt.title('Histogram of Nouns in Book 2');

fig = plt.gcf() 
fig.set_size_inches(10,4)
g = sns.histplot(x = vt1 ,color = "red" , kde = True)
plt.setp(g.get_xticklabels() , rotation = 90)
plt.xlabel('Different Categories of Verb in Book 1')
plt.ylabel('Frequency of each categories')
plt.title('Histogram of Verbs in Book 1');

fig = plt.gcf()
fig.set_size_inches(10,4)
g = sns.histplot(x = vt2 , color= "orange" , kde = True)
plt.setp(g.get_xticklabels() , rotation = 90)
plt.xlabel('Different Categories of Verb in Book 2')
plt.ylabel('Frequency of each categories')
plt.title('Histogram of Verbs in Book 2');

import spacy
ner = spacy.load('en_core_web_sm')

book1 = T1
entity_tag_book1 = []
lst1 = ner(book1)
for word in lst1.ents:
  x = word.text
  y = word.label_
  entity_tag_book1.append([x,y])

entity_tag_book1[:10]

book2 = T2
entity_tag_book2 = []
lst2 = ner(book2)
for word in lst2.ents:
  x = word.text
  y = word.label_
  entity_tag_book2.append([x,y])

entity_tag_book2[:10]

s3 = nerT1[:3000]
s3

entity_tag_passage_book1 = []
lst3 = ner(s3)
for word in lst3.ents:
  x = word.text
  y = word.label_
  entity_tag_passage_book1.append([x,y])

entity_tag_passage_book1

s4 = nerT2[:3000]
s4

entity_tag_passage_book2 = []
lst4 = ner(s4)
for word in lst4.ents:
  x = word.text
  y = word.label_
  entity_tag_passage_book2.append([x,y])

entity_tag_passage_book2

import re 
import string 
import nltk 
import spacy 
import pandas as pd 
import numpy as np 
import math 
from tqdm import tqdm 

from spacy.matcher import Matcher 
from spacy.tokens import Span 
from spacy import displacy 

pd.set_option('display.max_colwidth', 200)

# load spaCy model
nlp = spacy.load("en_core_web_sm")

text =nerT1

# create a spaCy object 
doc = nlp(text)

doc[:100]

#define the pattern 
pattern = [ 
           
           {'POS':'NOUN'}, 
           {'LOWER': 'agent', 'OP':"?"}, 
           {'LOWER': 'of'}, 
           {'POS':'DET'}, 
           {'POS': 'NOUN'}]

# Matcher class object 
matcher = Matcher(nlp.vocab) 
matcher.add("matching_1", None, pattern) 

matches = matcher(doc)

len(matches)

for i in range(20):
  span = doc[matches[i][1]:matches[i][2]] 
  print(span)

#define the pattern 
pattern = [
           {'POS':'NOUN'}, 
           {'LOWER': 'and'}, 
            
            
           {'POS': 'NOUN'}] 
           
matcher1 = Matcher(nlp.vocab) 
matcher1.add("matching_1", None, pattern) 

matches = matcher1(doc) 
print(len(matches))

for i in range(30):
  span = doc[matches[i][1]:matches[i][2]] 
  print(span)

pattern = [ {'ENT_TYPE':'PERSON'},
        
            
            {'LOWER': 'and','OP':"?"},
            {'POS':'DET'},
           {'POS':'NOUN'}]

matcher2 = Matcher(nlp.vocab) 
matcher2.add("matching_1", None, pattern) 

matches = matcher2(doc) 
print(len(matches))

for i in range(27):
  span = doc[matches[i][1]:matches[i][2]] 
  print(span)

pattern = [ {'ENT_TYPE':'PERSON','OP':"?"},
        
            
            {'LOWER': 'and','OP':"?"},
            {'POS':'DET'},
           {'ENT_TYPE':'GPE'},
           {'POS':'NOUN','OP':"?"}] 

matcher3 = Matcher(nlp.vocab) 
matcher3.add("matching_1", None, pattern) 

matches = matcher3(doc) 
print(len(matches))

for i in range(8):
  span = doc[matches[i][1]:matches[i][2]] 
  print(span)

pattern = [
           
           {'ENT_TYPE':'PERSON'},
           {'POS':'DET','OP':"?"},
           {'LOWER': 'has', 'OP':"?"},
           {'LOWER': 'lives', 'OP':"?"},
           {'ENT_TYPE':'GPE','OP':"?"},
           {'ENT_TYPE':'ORG','OP':"?"},
           {'LOWER': 'on', 'OP':"?"},
           {'POS':'NOUN'}]

#matcher.add("matching_1", None, pattern) 
matcher5 = Matcher(nlp.vocab) 
matcher5.add("matching_1", None, pattern) 

#matches = matcher(doc)
matches = matcher5(doc) 
print(len(matches))

for i in range(50):
  span = doc[matches[i][1]:matches[i][2]] 
  print(span)

3# Matcher class object 
matcher2 = Matcher(nlp.vocab)

#define the pattern 
pattern = [{'ENT_TYPE':'PERSON'}, 
           {'POS':'NOUN','OP':"?"}, 
           {'POS':'NOUN','OP':"?"}, 
            
            {'POS':'DET','OP':"?"},
           {'POS':'VERB'}
           ] 
           
matcher2.add("matching_1", None, pattern) 

matches = matcher2(doc)
print(len(matches))

for i in range(30):
  span = doc[matches[i][1]:matches[i][2]] 
  print(span)

matcher2 = Matcher(nlp.vocab)

#define the pattern 
pattern = [ {'POS':'PRON'}, 
           {'POS':'NOUN'}, 
            
            {'POS':'DET','OP':"?"},
           {'POS':'VERB'},
        {'POS':'NOUN','OP':"?"}, 
           
           ] 
           
matcher2.add("matching_1", None, pattern) 

matches = matcher2(doc)
matches

for i in range(len(matches)):
  span = doc[matches[i][1]:matches[i][2]] 
  print(span)

text =nerT2

# create a spaCy object 
doc = nlp(text)

doc[:300]

pattern = [ {'ENT_TYPE':'PERSON'},
        
            
            {'LOWER': 'and','OP':"?"},
            {'POS':'DET'},
           {'POS':'NOUN'}]

matcherT2 = Matcher(nlp.vocab) 
matcherT2.add("matching_1", None, pattern) 

matches = matcherT2(doc) 
print(len(matches))

for i in range(5):
  span = doc[matches[i][1]:matches[i][2]] 
  print(span)

pattern = [ {'ENT_TYPE':'PERSON','OP':"?"},
        
            
            {'LOWER': 'and','OP':"?"},
            {'POS':'DET'},
           {'ENT_TYPE':'GPE'},
           {'POS':'NOUN','OP':"?"}] 

matcherT3 = Matcher(nlp.vocab) 
matcherT3.add("matching_1", None, pattern) 

matches = matcherT3(doc) 
print(len(matches))

for i in range(6):
  span = doc[matches[i][1]:matches[i][2]] 
  print(span)

pattern = [
           
           {'ENT_TYPE':'PERSON'},
           {'POS':'DET','OP':"?"},
           {'LOWER': 'has', 'OP':"?"},
           {'LOWER': 'lives', 'OP':"?"},
           {'ENT_TYPE':'GPE','OP':"?"},
           {'ENT_TYPE':'ORG','OP':"?"},
           {'LOWER': 'on', 'OP':"?"},
           {'POS':'NOUN'}]

#matcher.add("matching_1", None, pattern) 
matcherT5 = Matcher(nlp.vocab) 
matcherT5.add("matching_1", None, pattern) 


matches = matcherT5(doc) 
print(len(matches))

for i in range(49):
  span = doc[matches[i][1]:matches[i][2]] 
  print(span)

3# Matcher class object 
matcherT6 = Matcher(nlp.vocab)

#define the pattern 
pattern = [{'ENT_TYPE':'PERSON'}, 
           {'POS':'NOUN','OP':"?"}, 
           {'POS':'NOUN','OP':"?"}, 
            
            {'POS':'DET','OP':"?"},
           {'POS':'VERB'},
           {'POS':'NOUN','OP':"?"}
           ] 
           
matcherT6.add("matching_1", None, pattern) 

matches = matcherT6(doc)
print(len(matches))

for i in range(30):
  span = doc[matches[i][1]:matches[i][2]] 
  print(span)

matcherT7 = Matcher(nlp.vocab)

#define the pattern 
pattern = [ {'POS':'PRON'}, 
           {'POS':'NOUN'}, 
            
            {'POS':'DET','OP':"?"},
           {'POS':'VERB'},
        {'POS':'NOUN','OP':"?"}, 
           
           ] 
           
matcherT7.add("matching_1", None, pattern) 

matches = matcherT7(doc)
len(matches)

for i in range(len(matches)):
  span = doc[matches[i][1]:matches[i][2]] 
  print(span)

